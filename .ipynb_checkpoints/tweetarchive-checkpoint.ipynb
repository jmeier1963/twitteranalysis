{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of tweet archive\n",
    "As the Twitter API only maintains a 7-day search index one needs to build an archive of tweets manually.\n",
    "Most of the code is adapted from https://github.com/Jefferson-Henrique/GetOldTweets-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error,urllib.request,urllib.error,urllib.parse,json,re,datetime,sys,http.cookiejar\n",
    "from pyquery import PyQuery\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import heapq\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "# search parameters German or English\n",
    "q = '#Suffizienz'\n",
    "q1 = q[1:].lower()\n",
    "q2 = q1.capitalize()\n",
    "\n",
    "since = \"2010-01-01\"\n",
    "until = \"2020-6-30\"\n",
    "\n",
    "bins = 20\n",
    "    \n",
    "stop_ger = nltk.corpus.stopwords.words('german')\n",
    "stop_eng = nltk.corpus.stopwords.words('english')\n",
    "customstopwords = [q1, q2, 'dlvr', 'az', 'su', 'html', 'twitter', 'aw', 'com', 'pic', 'bit', 'via', 'https']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an archive of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetCriteria:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.maxTweets = 0\n",
    "\n",
    "\tdef setUsername(self, username):\n",
    "\t\tself.username = username\n",
    "\t\treturn self\n",
    "\n",
    "\tdef setSince(self, since):\n",
    "\t\tself.since = since\n",
    "\t\treturn self\n",
    "\n",
    "\tdef setUntil(self, until):\n",
    "\t\tself.until = until\n",
    "\t\treturn self\n",
    "\n",
    "\tdef setQuerySearch(self, querySearch):\n",
    "\t\tself.querySearch = querySearch\n",
    "\t\treturn self\n",
    "\n",
    "\tdef setMaxTweets(self, maxTweets):\n",
    "\t\tself.maxTweets = maxTweets\n",
    "\t\treturn self\n",
    "\n",
    "\tdef setLang(self, Lang):\n",
    "\t\tself.lang = Lang\n",
    "\t\treturn self\n",
    "\n",
    "\tdef setTopTweets(self, topTweets):\n",
    " \t\tself.topTweets = topTweets\n",
    " \t\treturn self\n",
    "    \n",
    "class Tweet:\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "def getTweets(tweetCriteria, receiveBuffer=None, bufferLength=100, proxy=None):\n",
    "    refreshCursor = ''\n",
    "\n",
    "    results = []\n",
    "    resultsAux = []\n",
    "    cookieJar = http.cookiejar.CookieJar()\n",
    "\n",
    "    active = True\n",
    "\n",
    "    while active:\n",
    "        json = getJsonReponse(tweetCriteria, refreshCursor, cookieJar, proxy)\n",
    "        if len(json['items_html'].strip()) == 0:\n",
    "            break\n",
    "\n",
    "        refreshCursor = json['min_position']\n",
    "        scrapedTweets = PyQuery(json['items_html'])\n",
    "        #Remove incomplete tweets withheld by Twitter Guidelines\n",
    "        scrapedTweets.remove('div.withheld-tweet')\n",
    "        tweets = scrapedTweets('div.js-stream-tweet')\n",
    "\n",
    "        if len(tweets) == 0:\n",
    "            break\n",
    "        \n",
    "        for tweetHTML in tweets:\n",
    "            tweetPQ = PyQuery(tweetHTML)            \n",
    "            \n",
    "            # usernameTweet = tweetPQ(\"span.username.js-action-profile-name b\").text()  does not work\n",
    "            usernames = tweetPQ(\"span.username\").text()   # gets multiple usernames\n",
    "            usernameTweet = re.sub(\"[^\\w]\", \" \",  usernames).split()[0]   # pick first name in list   \n",
    "            s = re.sub(r\"\\s+\", \" \", tweetPQ(\"p.js-tweet-text\").text())\n",
    "            txt = s.replace('# ', '#').replace('@ ', '@')   \n",
    "            retweets = int(tweetPQ(\"span.ProfileTweet-action--retweet span.ProfileTweet-actionCount\").attr(\"data-tweet-stat-count\").replace(\",\", \"\"))\n",
    "            favorites = int(tweetPQ(\"span.ProfileTweet-action--favorite span.ProfileTweet-actionCount\").attr(\"data-tweet-stat-count\").replace(\",\", \"\"))\n",
    "            dateSec = int(tweetPQ(\"small.time span.js-short-timestamp\").attr(\"data-time\"))\n",
    "            id = tweetPQ.attr(\"data-tweet-id\")\n",
    "            permalink = tweetPQ.attr(\"data-permalink-path\")\n",
    "            user_id = int(tweetPQ(\"a.js-user-profile-link\").attr(\"data-user-id\"))      \n",
    "            geo = ''\n",
    "            geoSpan = tweetPQ('span.Tweet-geo')\n",
    "            if len(geoSpan) > 0:\n",
    "                geo = geoSpan.attr('title')           \n",
    "            urls = []\n",
    "            for link in tweetPQ(\"a\"):\n",
    "                try:\n",
    "                    urls.append((link.attrib[\"data-expanded-url\"]))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            \n",
    "            tweet = {}         \n",
    "            tweet[\"id\"] = id\n",
    "            tweet[\"permalink\"] = 'https://twitter.com' + permalink\n",
    "            tweet[\"username\"] = usernameTweet\n",
    "            tweet[\"text\"] = txt\n",
    "            tweet[\"date\"] = datetime.datetime.fromtimestamp(dateSec)\n",
    "            tweet[\"formatted_date\"] = datetime.datetime.fromtimestamp(dateSec).strftime(\"%a %b %d %X +0000 %Y\")\n",
    "            tweet[\"retweets\"] = retweets\n",
    "            tweet[\"favorites\"] = favorites\n",
    "            tweet[\"mentions\"] = \" \".join(re.compile('(@\\\\w*)').findall(txt))\n",
    "            tweet[\"hashtags\"] = \" \".join(re.compile('(#\\\\w*)').findall(txt))\n",
    "            tweet[\"geo\"] = geo\n",
    "            tweet[\"urls\"] = \",\".join(urls)\n",
    "            tweet[\"author_id\"] = user_id\n",
    "            \n",
    "            results.append(tweet)\n",
    "            resultsAux.append(tweet)\n",
    "            \n",
    "            if receiveBuffer and len(resultsAux) >= bufferLength:\n",
    "                receiveBuffer(resultsAux)\n",
    "                resultsAux = []\n",
    "            \n",
    "            if tweetCriteria.maxTweets > 0 and len(results) >= tweetCriteria.maxTweets:\n",
    "                active = False\n",
    "                break\n",
    "\n",
    "    if receiveBuffer and len(resultsAux) > 0:\n",
    "        receiveBuffer(resultsAux)\n",
    "\n",
    "    return results\n",
    "\n",
    "def getJsonReponse(tweetCriteria, refreshCursor, cookieJar, proxy):\n",
    "    url = \"https://twitter.com/i/search/timeline?f=tweets&q=%s&src=typd&%smax_position=%s\"\n",
    "    \n",
    "    urlGetData = ''\n",
    "    if hasattr(tweetCriteria, 'username'):\n",
    "        urlGetData += ' from:' + tweetCriteria.username\n",
    "    if hasattr(tweetCriteria, 'since'):\n",
    "        urlGetData += ' since:' + tweetCriteria.since\n",
    "    if hasattr(tweetCriteria, 'until'):\n",
    "        urlGetData += ' until:' + tweetCriteria.until\n",
    "    if hasattr(tweetCriteria, 'querySearch'):\n",
    "        urlGetData += ' ' + tweetCriteria.querySearch\n",
    "    if hasattr(tweetCriteria, 'lang'):\n",
    "        urlLang = 'lang=' + tweetCriteria.lang + '&'\n",
    "    else:\n",
    "        urlLang = ''\n",
    "    url = url % (urllib.parse.quote(urlGetData), urlLang, refreshCursor)\n",
    "    #print(url)\n",
    "\n",
    "    headers = [\n",
    "        ('Host', \"twitter.com\"),\n",
    "        ('User-Agent', \"Mozilla/5.0 (Windows NT 6.1; Win64; x64)\"),\n",
    "        ('Accept', \"application/json, text/javascript, */*; q=0.01\"),\n",
    "        ('Accept-Language', \"de,en-US;q=0.7,en;q=0.3\"),\n",
    "        ('X-Requested-With', \"XMLHttpRequest\"),\n",
    "        ('Referer', url),\n",
    "        ('Connection', \"keep-alive\")\n",
    "        ]\n",
    "\n",
    "    if proxy:\n",
    "        opener = urllib.request.build_opener(urllib.request.ProxyHandler({'http': proxy, 'https': proxy}), urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    else:\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    opener.addheaders = headers\n",
    "\n",
    "    try:\n",
    "        response = opener.open(url)\n",
    "        jsonResponse = response.read()\n",
    "    except:\n",
    "        #print(\"Twitter weird response. Try to see on browser: \", url)\n",
    "        print(\"Twitter weird response. Try to see on browser: https://twitter.com/search?q=%s&src=typd\" % urllib.parse.quote(urlGetData))\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        sys.exit()\n",
    "        return\n",
    "\n",
    "    dataJson = json.loads(jsonResponse.decode())\n",
    "\n",
    "    return dataJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_interval(q, since, until):\n",
    "    tweetCriteria = TweetCriteria().setQuerySearch(q).setSince(since).setUntil(until).setMaxTweets(100)\n",
    "    tweets = getTweets(tweetCriteria)\n",
    "    if len(tweets) == 100:\n",
    "        s = time.mktime(datetime.datetime.strptime(since, \"%Y-%m-%d\").timetuple())\n",
    "        u = time.mktime(datetime.datetime.strptime(until, \"%Y-%m-%d\").timetuple())\n",
    "        d = datetime.datetime.utcfromtimestamp((s + u)/2)\n",
    "        middle_time = d.strftime(\"%Y-%m-%d\")\n",
    "        print (middle_time)\n",
    "        tweets1 = tweet_interval(q, since, middle_time)\n",
    "        tweets2 = tweet_interval(q, middle_time, until)\n",
    "        if (len(tweets1)+len(tweets2) == 100):\n",
    "            return tweets\n",
    "        else: \n",
    "            return tweets1 + tweets2\n",
    "    else:\n",
    "        return tweets\n",
    "\n",
    "    \n",
    "s = time.mktime(datetime.datetime.strptime(since, \"%Y-%m-%d\").timetuple())\n",
    "u = time.mktime(datetime.datetime.strptime(until, \"%Y-%m-%d\").timetuple())\n",
    "\n",
    "bin_size = (u-s)/bins\n",
    "bin_counter = 1\n",
    "\n",
    "while s < u:\n",
    "    new_since = datetime.datetime.utcfromtimestamp(s).strftime(\"%Y-%m-%d\")\n",
    "    if (s + bin_size < u):\n",
    "        new_until = datetime.datetime.utcfromtimestamp(s + bin_size).strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        new_until = datetime.datetime.utcfromtimestamp(u).strftime(\"%Y-%m-%d\")\n",
    "    print(\"Gathering tweets from \", new_since, \"to\", new_until)\n",
    "    tweets = tweet_interval(q, new_since, new_until)\n",
    "    pd.DataFrame(tweets).to_pickle(q+\"_tweets_bin\"+str(bin_counter)+\".pkl\")\n",
    "    bin_counter += 1\n",
    "    s += bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "filelist = [ f for f in os.listdir(cwd) if f.startswith(q+\"_tweets_bin\") ]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df_list = []\n",
    "for f in filelist:\n",
    "    df_list.append(pd.read_pickle(f))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index= True)\n",
    "df.to_pickle(q + \"_tweets_\" + since + \"_to_\" + until +\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "filelist = [ f for f in os.listdir(cwd) if f.startswith(q+\"_tweets_bin\") ]\n",
    "for f in filelist:                                                              \n",
    "    os.remove(os.path.join(cwd, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets per month containing search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweets\"]=df[\"id\"].apply(lambda x: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(pd.DatetimeIndex(df['date']))\n",
    "df.groupby(pd.Grouper(freq='M')).sum()[\"tweets\"].plot(figsize=(16,12))\n",
    "plt.savefig(q+\"-tweetspermonth.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution Tweets per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.groupby(\"username\")[\"id\"].count().sort_values(ascending=False).plot(figsize=(16,12),title=\"Tweets per user\")\n",
    "plt.savefig(q+\"-handles.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"username\")[\"id\"].count().sort_values(ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network of links between users (i.e. mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "\n",
    "network = networkx.Graph()\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "\n",
    "for index, status in df.iterrows():\n",
    "    if status[\"username\"] not in network.nodes:\n",
    "        network.add_node(status[\"username\"], name=status[\"username\"])\n",
    "    \n",
    "    for username in status['mentions'].split():\n",
    "        shortname = username[1:]     # strip @\n",
    "        if shortname not in network.nodes:\n",
    "            network.add_node(shortname, name=shortname)\n",
    "  \n",
    "        if (status[\"username\"], shortname) in network.edges:\n",
    "            network[status[\"username\"]][shortname]['weight'] +=1\n",
    "            c1 += 1\n",
    "        else:\n",
    "            network.add_edge(status[\"username\"], shortname, weight=1.0)\n",
    "            c2 += 1\n",
    "            \n",
    "print(network.number_of_nodes(), \"Twitterusers in Tweets where\", q, \"is mentioned.\")\n",
    "print(c2+c1, \"cross-references among those users with\", c1, \"unique edges.\")\n",
    "\n",
    "networkx.write_gml(network, q+\".gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import show, figure\n",
    "from bokeh.io import export_png\n",
    "from math import sqrt\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "network = networkx.read_gml(q+'.gml')\n",
    "\n",
    "layout = networkx.spring_layout(network, k=1.1/sqrt(network.number_of_nodes()), iterations=100)\n",
    "# https://en.wikipedia.org/wiki/Force-directed_graph_drawing\n",
    "nodes, nodes_coordinates = zip(*sorted(layout.items()))\n",
    "nodes_xs, nodes_ys = list(zip(*nodes_coordinates))\n",
    "nodes_source = ColumnDataSource(dict(x=nodes_xs, y=nodes_ys, name=nodes))\n",
    "\n",
    "hover = HoverTool(tooltips=[('name', '@name'), ('id', '$index')])\n",
    "plot = figure(plot_width=1200, plot_height=800, tools=['tap', hover, 'box_zoom', 'reset'])\n",
    "r_circles = plot.circle('x', 'y', source=nodes_source, size=10, color='blue', level = 'overlay')\n",
    "\n",
    "def get_edges_specs(_network, _layout):\n",
    "    d = dict(xs=[], ys=[], alphas=[])\n",
    "    weights = [d['weight'] for u, v, d in _network.edges(data=True)]\n",
    "    max_weight = max(weights)\n",
    "    calc_alpha = lambda h: 0.1 + 0.6 * (h / max_weight)\n",
    "\n",
    "    # example: { ..., ('user47', 'da_bjoerni', {'weight': 3}), ... }\n",
    "    for u, v, data in _network.edges(data=True):\n",
    "        d['xs'].append([_layout[u][0], _layout[v][0]])\n",
    "        d['ys'].append([_layout[u][1], _layout[v][1]])\n",
    "        d['alphas'].append(calc_alpha(data['weight']))\n",
    "    return d\n",
    "\n",
    "lines_source = ColumnDataSource(get_edges_specs(network, layout))\n",
    "r_lines = plot.multi_line('xs', 'ys', line_width=1.5, alpha='alphas', color='navy', source=lines_source)\n",
    "\n",
    "centrality = networkx.algorithms.centrality.betweenness_centrality(network)\n",
    "# first element are nodes again\n",
    "_, nodes_centrality = zip(*sorted(centrality.items()))\n",
    "max_centrality = max(nodes_centrality)\n",
    "\n",
    "nodes_source.add([7 + 10 * t / max_centrality for t in nodes_centrality], 'centrality')\n",
    "\n",
    "r_circles.glyph.size = 'centrality'\n",
    "\n",
    "import community # python-louvain\n",
    "partition = community.best_partition(network)\n",
    "p_, nodes_community = zip(*sorted(partition.items()))\n",
    "nodes_source.add(nodes_community, 'community')\n",
    "community_colors = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33','#a65628', '#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666']\n",
    "nodes_source.add([community_colors[t % len(community_colors)] for t in nodes_community], 'community_color')\n",
    "\n",
    "r_circles.glyph.fill_color = 'community_color'\n",
    "\n",
    "export_png(plot, filename=q+\"-network.png\")\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution key terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean the text with special characters, extra spaces etc..'''\n",
    "    clean_abstract = re.sub(r'\\d',' ',text)\n",
    "    clean_abstract = re.sub(r'\\W', ' ',clean_abstract)    \n",
    "    clean_abstract = re.sub(r'\\s+', ' ',clean_abstract)\n",
    "    clean_abstract = re.sub(r'\\[[0-9]*\\]',' ', clean_abstract)\n",
    "    return clean_abstract\n",
    "\n",
    "rawtext = \" \".join(df[\"text\"]).lower()\n",
    "rawtext = clean_text(rawtext)\n",
    "tokens = []\n",
    "sentences = []\n",
    "for txt in rawtext.split():\n",
    "    tokens.extend([t.lower().strip(\"\\':,.!?\") for t in txt.split()])\n",
    "\n",
    "hashtags = [w for w in tokens if w.startswith('#')]\n",
    "mentions = [w for w in tokens if w.startswith('@')]\n",
    "links = [w for w in tokens if w.startswith('http') or w.startswith('www')]\n",
    "filtered_tokens = [w for w in tokens \\\n",
    "                   if not w in stop_eng \\\n",
    "                   and not w in stop_ger \\\n",
    "                   and not w in customstopwords \\\n",
    "                   and w.isalpha() \\\n",
    "                   and not len(w)<3 \\\n",
    "                   and not w in hashtags \\\n",
    "                   and not w in links \\\n",
    "                   and not w in mentions]\n",
    "filtered_keywords = \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(filtered_tokens)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(size=16)\n",
    "plt.title(\"Key word frequency\")\n",
    "freq_dist.plot(40)\n",
    "plt.savefig(q+\"-keyterms.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(text, filename):\n",
    "    '''Word cloud'''\n",
    "    wordcloud = WordCloud(max_font_size=1000, max_words=50, background_color=\"white\").generate(str(text))\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.imshow(wordcloud, interpolation= 'bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(filename+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(filtered_keywords,q+\"wc_keyterms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ListHashtags = \" \".join(df[\"hashtags\"]).lower().split()\n",
    "\n",
    "a = dict(Counter(ListHashtags))\n",
    "\n",
    "sorted(a, key=a.get, reverse=True)[:50]\n",
    "# for w in sorted(a, key=a.get, reverse=True): print(w, a[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(list(filter(lambda a: a != q1, ListHashtags)))\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(size=16)\n",
    "plt.title(\"Hashtag frequency\")\n",
    "freq_dist.plot(40)\n",
    "plt.savefig(q+\"-hashtagfrequency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud Hashtags without #\n",
    "s = \" \".join(df[\"hashtags\"])\n",
    "c = clean_text(s)\n",
    "c = c.replace(q1, \"\")\n",
    "c = c.replace(q2, \"\")\n",
    "wordcloud(c, q+\"wc_hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
